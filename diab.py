# -*- coding: utf-8 -*-
"""GANNLP (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UiVpNUxZqI-lDvaWE84Sep2vVnv-xsWZ

## **Data Preprocessing Using NLP**
"""

# Step 1: Data Preprocessing
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Load the dataset
data = pd.read_csv('Diabetes.csv')
data.head()

# Check for missing values
print(data.isnull().sum())

# Handle missing values (assuming forward fill for simplicity)
data.fillna(method='ffill', inplace=True)

# Drop any remaining NaN values if present
data.dropna(inplace=True)

"""# **Building Attention-based GAN for Data Augmentation**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input, Reshape, Flatten, Dot, Add
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Define the dimensions of the input noise vector
latent_dim = 100

# Define the attention layer
def attention_layer(x):
  # Attention mechanism with a single attention head
  attention_weights = Dense(128, activation='softmax')(x)  # Change the shape to match x
  attention_weights = Reshape((x.shape[-1],))(attention_weights)  # Reshape to match x shape
  context_vector = Dot(axes=1)([attention_weights, x])
  output = Add()([x, context_vector])  # Add weighted features to original input
  return output

# Define the generator model with attention
def build_generator(latent_dim):
  input_noise = Input(shape=(latent_dim,))
  x = Dense(128, activation='relu')(input_noise)
  x = attention_layer(x)  # Apply attention layer
  x = Dense(256, activation='relu')(x)
  x = Dense(512, activation='relu')(x)
  output = Dense(data.shape[1], activation='sigmoid')(x)  # Output layer matches data dimension
  generator = Model(inputs=input_noise, outputs=output)
  return generator

# Define the discriminator model
def build_discriminator(input_shape):
    input_data = Input(shape=input_shape)
    x = Dense(512, activation='relu')(input_data)
    x = Dense(256, activation='relu')(x)
    x = Dense(128, activation='relu')(x)
    output = Dense(1, activation='sigmoid')(x)
    discriminator = Model(inputs=input_data, outputs=output)
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])
    return discriminator

# Compile the discriminator
discriminator = build_discriminator(input_shape=(data.shape[1],))
discriminator.trainable = False

# Build the GAN model
generator = build_generator(latent_dim)
gan_input = Input(shape=(latent_dim,))
synthetic_data = generator(gan_input)
gan_output = discriminator(synthetic_data)
gan = Model(gan_input, gan_output)
gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

# Training the GAN
def train_gan(data, generator, discriminator, gan, latent_dim, epochs=500, batch_size=128):
    for epoch in range(epochs):
        # Sample random noise for generator input
        noise = np.random.normal(0, 1, (batch_size, latent_dim))

        # Generate synthetic data
        generated_data = generator.predict(noise)

        # Select a random batch of real data
        idx = np.random.randint(0, data.shape[0], batch_size)
        real_data = data[idx]

        # Train the discriminator
        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))
        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator (via the GAN model)
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        valid_labels = np.ones((batch_size, 1))
        g_loss = gan.train_on_batch(noise, valid_labels)

        # Print progress
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}")

# Train the GAN
train_gan(data.values.astype('float32'), generator, discriminator, gan, latent_dim)

import numpy as np
import pandas as pd
import random
from sklearn.model_selection import train_test_split

# Assuming 'data' is your original dataset and 'generator' is your trained generator model
# Define the number of synthetic samples to generate
num_synthetic_samples = len(data)  # Generate as many synthetic samples as the original data

# Generate random noise for the generator input
noise = np.random.normal(0, 1, (num_synthetic_samples, latent_dim))

# Generate synthetic data
synthetic_data = generator.predict(noise)

# Apply thresholding to convert probabilities to binary values
# Assuming the target column is the first column (index 0)
# Adjust the threshold if necessary to achieve a balanced distribution
threshold = 0.5  # Adjust this value as needed
synthetic_data[:, 0] = (synthetic_data[:, 0] > threshold).astype(int)

# Convert synthetic_data to DataFrame
synthetic_df = pd.DataFrame(synthetic_data)

# Now, synthetic_df contains the generated synthetic data with binary values in the target column
print(synthetic_df.head())

import numpy as np
import pandas as pd

# Assuming 'data' is your original dataset and 'generator' is your trained generator model
# Define the number of synthetic samples to generate
num_synthetic_samples = len(data)  # Generate as many synthetic samples as the original data

# Generate random noise for the generator input
noise = np.random.normal(0, 1, (num_synthetic_samples, latent_dim))

# Generate synthetic data
synthetic_data = generator.predict(noise)

# Generate binary values for the y column
y_values = np.random.choice([0, 1], size=num_synthetic_samples)

# Assuming the first column of synthetic_data is the target column (y)
# and the rest are x values
# Combine y_values with the x values from synthetic_data
combined_data = np.column_stack((y_values, synthetic_data[:, 1:]))

# Define the column names
column_names = ['Diabetes_012', 'HighBP', 'HighChol', 'BMI', 'Smoker', 'Stroke', 'PhysActivity', 'Fruits', 'HvyAlcoholConsump', 'AnyHealthcare', 'Sex', 'Age']

# Create the DataFrame with the specified column names
synthetic_df = pd.DataFrame(combined_data, columns=column_names)

# Now, synthetic_df contains the generated synthetic data with binary y values and x values
print(synthetic_df.head())

import numpy as np
import pandas as pd

# Convert synthetic_data_array to DataFrame and assign original column names
synthetic_data = pd.DataFrame(synthetic_df, columns=data.columns)

# Now, synthetic_data contains the generated synthetic data with the original column names
print(synthetic_data.head())

# Define the file path where you want to save the CSV file
csv_file_path = "synthetic_data.csv"

# Save the DataFrame to a CSV file
synthetic_df.to_csv(csv_file_path, index=False)

print("Synthetic data saved to:", csv_file_path)

synthetic_data = pd.read_csv("synthetic_data.csv")

"""# **Logistic Regression for Synthetic  Dataset**"""

# Separate the target variable
y_syn = synthetic_data.iloc[:, 0]

# Create a copy of the data without the target variable
X_syn = synthetic_data.iloc[:, 1:]

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(X_syn, y_syn, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_syn)
X_test_scaled = scaler.transform(X_test_syn)

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train_syn)

# Assuming you already have y_test
y_pred_syn = log_reg.predict(X_test_scaled)

accuracy_syn = accuracy_score(y_test_syn, y_pred_syn)
print("Accuracy of logistic regression model trained with Synthetic Dataset:", accuracy_syn)

conf_matrix = confusion_matrix(y_test_syn, y_pred_syn)
print("Confusion Matrix:")
print(conf_matrix)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'data' is your original dataset and 'synthetic_data' is your synthetic dataset
# Reset the index of synthetic_data to ensure consistent indexing
synthetic_data.reset_index(drop=True, inplace=True)

# Select the first 1000 samples from your original data
selected_original_data = data.iloc[:1000]

# Select a random sample of the synthetic data
num_synthetic_samples_to_select = 1000  # Adjust this number as needed
selected_synthetic_data_indices = np.random.choice(len(synthetic_data), num_synthetic_samples_to_select, replace=False)
selected_synthetic_data = synthetic_data.loc[selected_synthetic_data_indices]

# Combine selected original data with selected synthetic data
augmented_data = np.concatenate([selected_original_data.values, selected_synthetic_data.values], axis=0)

# Create labels for the augmented dataset
# Assuming the original data is labeled as 1 and synthetic data as 0
augmented_labels = np.concatenate([np.ones(1000), np.zeros(num_synthetic_samples_to_select)])

# Shuffle the augmented dataset
shuffled_indices = np.random.permutation(len(augmented_data))
augmented_data = augmented_data[shuffled_indices]
augmented_labels = augmented_labels[shuffled_indices]

# Split the augmented dataset into train and test sets
X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(augmented_data, augmented_labels, test_size=0.2, random_state=42)

auggg = pd.DataFrame(augmented_data)
# Assuming synthetic_df is your DataFrame
auggg.head()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import numpy as np

# Assuming 'auggg' is your DataFrame and 'y_ag' is your target variable
y_ag = auggg.iloc[:, 0]
X_ag = auggg.iloc[:, 1:]

# Scale the features
scaler = StandardScaler()
X_ag_scaled = scaler.fit_transform(X_ag)

# Initialize the LogisticRegression model
log_reg = LogisticRegression(max_iter=1000)

# Perform cross-validation
cv_scores = cross_val_score(log_reg, X_ag_scaled, y_ag, cv=10, scoring='accuracy')

# Calculate the mean accuracy across all folds
mean_accuracy = np.mean(cv_scores)
print(f"Mean Accuracy of the LogisticRegression model: {mean_accuracy:.2f}")

# Fit the model on the entire dataset (not necessary if using cross-validation)
log_reg.fit(X_ag_scaled, y_ag)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import numpy as np

# Assuming 'auggg' is your DataFrame and 'y_ag' is your target variable
y_ag = auggg.iloc[:, 0]
X_ag = auggg.iloc[:, 1:]

# Scale the features
scaler = StandardScaler()
X_ag_scaled = scaler.fit_transform(X_ag)

# Initialize the RandomForestClassifier model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(rf, X_ag_scaled, y_ag, cv=10, scoring='accuracy')

# Calculate the mean accuracy across all folds
mean_accuracy = np.mean(cv_scores)
print(f"Mean Accuracy of the RandomForestClassifier model: {mean_accuracy:.2f}")

# Fit the model on the entire dataset (not necessary if using cross-validation)
rf.fit(X_ag_scaled, y_ag)

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import Matern
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming 'auggg' is your DataFrame and 'y_ag' is your target variable
y_ag = auggg.iloc[:, 0]
X_ag = auggg.iloc[:, 1:]

# Scale the features
scaler = StandardScaler()
X_ag_scaled = scaler.fit_transform(X_ag)

# Personalized risk assessment using Gaussian Process Classifier with Mat√©rn kernel
gpc_m = GaussianProcessClassifier(kernel=1.0 * RBF(1.0) + Matern(length_scale=2.0, nu=1.5))

# Fit the GPC model
gpc_m.fit(X_ag_scaled, y_ag)
from sklearn.metrics import accuracy_score

# Make predictions on the training data
y_pred = gpc_m.predict(X_ag_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_ag, y_pred)
print("Accuracy of GPC with Matern Kernel:", accuracy)

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import RationalQuadratic
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming 'auggg' is your DataFrame and 'y_ag' is your target variable
y_ag = auggg.iloc[:, 0]
X_ag = auggg.iloc[:, 1:]

# Scale the features
scaler = StandardScaler()
X_ag_scaled = scaler.fit_transform(X_ag)

# Personalized risk assessment using Gaussian Process Classifier
gpc_r = GaussianProcessClassifier(kernel=1.0 * RBF(1.0) + RationalQuadratic(alpha=0.1, length_scale=1.0))

# Fit the GPC model
gpc_r.fit(X_ag_scaled, y_ag)

from sklearn.metrics import accuracy_score

# Make predictions on the training data
y_pred = gpc_r.predict(X_ag_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_ag, y_pred)
print("Accuracy of GPC with Rational quadratic kernel:", accuracy)

import pickle

pickle.dump(log_reg,open('model.pkl','wb'))
model=pickle.load(open('model.pkl','rb'))